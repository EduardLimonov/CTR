{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3RLDpdY5fWG5h6dDQdKUZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c4cc201b5e874f50b4d465b466602a49":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0255e6687ad47b8b9fb70a04b960913","IPY_MODEL_4bbae8f4ccfd4555b7f94cc5c7fe09ee","IPY_MODEL_00484eaea7c146a2b4927d3f88c7455b"],"layout":"IPY_MODEL_2330944dcd7141b8a7bd93b18e7b9322"}},"e0255e6687ad47b8b9fb70a04b960913":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7c5c3007b45f4bb6861271d7d4a4bdac","placeholder":"​","style":"IPY_MODEL_d8787969600a4c648a0409fe47150ea4","value":"100%"}},"4bbae8f4ccfd4555b7f94cc5c7fe09ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e1448eeda484a51990ccd2aec3d3675","max":2700,"min":0,"orientation":"horizontal","style":"IPY_MODEL_141dee8fd8d4429c8a30449731528bc7","value":2700}},"00484eaea7c146a2b4927d3f88c7455b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_854ca7cc836a4cd4a77754c009c6dd0d","placeholder":"​","style":"IPY_MODEL_2889617388634a38bb72d5d54df24c81","value":" 2700/2700 [11:53&lt;00:00,  3.23it/s]"}},"2330944dcd7141b8a7bd93b18e7b9322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c5c3007b45f4bb6861271d7d4a4bdac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8787969600a4c648a0409fe47150ea4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e1448eeda484a51990ccd2aec3d3675":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"141dee8fd8d4429c8a30449731528bc7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"854ca7cc836a4cd4a77754c009c6dd0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2889617388634a38bb72d5d54df24c81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c82I0bTc0q7Y","executionInfo":{"status":"ok","timestamp":1679325848477,"user_tz":-180,"elapsed":26250,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"f84f1093-c9bc-40aa-f241-5421f33ba1a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install pytorch_lightning"],"metadata":{"id":"Nei0-KqVD_qK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679325861295,"user_tz":-180,"elapsed":12842,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"3d1060f4-bd55-45b5-fe83-3e5b30af2ec9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pytorch_lightning\n","  Downloading pytorch_lightning-2.0.0-py3-none-any.whl (715 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m715.6/715.6 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.5.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (1.22.4)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (1.13.1+cu116)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (23.0)\n","Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (2023.3.0)\n","Collecting lightning-utilities>=0.7.0\n","  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n","Collecting torchmetrics>=0.7.0\n","  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 KB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch_lightning) (4.65.0)\n","Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.27.1)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.2.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.0.12)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.4)\n","Installing collected packages: multidict, lightning-utilities, frozenlist, async-timeout, yarl, torchmetrics, aiosignal, aiohttp, pytorch_lightning\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 lightning-utilities-0.8.0 multidict-6.0.4 pytorch_lightning-2.0.0 torchmetrics-0.11.4 yarl-1.8.2\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn, tensor, Tensor\n","from tqdm.notebook import tqdm \n","from torch.utils.data import DataLoader\n","import h5py\n","import numpy as np\n","from typing import *\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt \n","import pytorch_lightning as pl\n","from torch.optim.lr_scheduler import *\n","import pickle"],"metadata":{"id":"YA9pmPRg2N-C","executionInfo":{"status":"ok","timestamp":1679325867361,"user_tz":-180,"elapsed":6080,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!pip install pythreshold"],"metadata":{"id":"AImSDCm9RDKk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679325878566,"user_tz":-180,"elapsed":11218,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"00fc5f01-7b37-4ac7-f1a6-9b19dcced7d6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pythreshold\n","  Downloading pythreshold-0.3.1.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from pythreshold) (1.22.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from pythreshold) (1.10.1)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (from pythreshold) (0.19.3)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from pythreshold) (3.7.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.9/dist-packages (from pythreshold) (4.6.0.66)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (8.4.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (1.0.7)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (4.39.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (23.0)\n","Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (5.12.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (0.11.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->pythreshold) (3.0.9)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->pythreshold) (1.4.1)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image->pythreshold) (3.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image->pythreshold) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image->pythreshold) (2023.3.15)\n","Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->pythreshold) (3.15.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->pythreshold) (1.15.0)\n","Building wheels for collected packages: pythreshold\n","  Building wheel for pythreshold (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pythreshold: filename=pythreshold-0.3.1-py3-none-any.whl size=25035 sha256=3f64d12ce6717de2151cfd8672a22e73fc80fefca1d50466718d748cc46b0c2a\n","  Stored in directory: /root/.cache/pip/wheels/c8/f2/0d/66117bc7a8742329e756cde86fe0deb45ddc9b8bc83b26f187\n","Successfully built pythreshold\n","Installing collected packages: pythreshold\n","Successfully installed pythreshold-0.3.1\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Функционал, используемый для обработки изображения.\n","\n","Функции view_image, view_images являются отладочными или используются для демонстрации работы алгоритмов.\n","\n","Класс PicHandler используется для перевода изображение в полутоновое представление (в градациях серого),\n","для его фильтрации, бинаризации (алгоритмы  адаптивной бинаризации), обрезки и т.д.\n","\"\"\"\n","\n","\n","from __future__ import annotations\n","from typing import *\n","import cv2\n","import numpy as np\n","from pythreshold.utils import *\n","#from utils.geometry import Rect\n","from skimage.transform import resize, rescale\n","import enum\n","\n","\n","class FilterType(enum.Enum):\n","    GAUSSIAN_FILTER = 0\n","    MEDIAN_FILTER = 1\n","\n","class StackingType(enum.Enum):\n","    HORIZONTAL = 0\n","    VERTICAL = 1\n","\n","\n","class Side(enum.Enum):\n","    left = 'left'\n","    right = 'right'\n","    top = 'top'\n","    bottom = 'bottom'\n","    all = 'all'\n","\n","\n","def view_image(image: np.ndarray, name_of_window: str = 'Image'):\n","    # выводит на экран изображение image (массив представления BGR)\n","    cv2.namedWindow(name_of_window, cv2.WINDOW_FULLSCREEN)\n","    cv2.imshow(name_of_window, image)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","\n","def view_images(images: Iterable[np.ndarray], name_of_window: str = 'Images', \n","                stacking: StackingType = StackingType.HORIZONTAL):\n","    # выводит на экран набор изображений images (массивы представлений BGR)\n","    cv2.namedWindow(name_of_window, cv2.WINDOW_NORMAL)\n","    res = images[0]\n","    if stacking == StackingType.HORIZONTAL:\n","        ax = 1\n","    else:\n","        ax = 0\n","    for i in range(1, len(images)):\n","        res = np.concatenate((res, images[i]), axis=ax)\n","\n","    cv2.imshow(name_of_window, res)\n","    cv2.waitKey(0)\n","    cv2.destroyAllWindows()\n","\n","\n","class PicHandler:\n","    img: np.ndarray  # объект изображения в BGR\n","\n","    def __init__(self, image: Union[str, np.ndarray], make_copy: bool = True, is_colored: bool = True):\n","        # image -- путь к файлу с изображением или np.ndarray -- представление изображения BGR в виде массива;\n","        # если передан массив, то make_copy: bool -- необходимо ли работать с копией переданного массива;\n","        # если передан путь к файлу, то изображение открывается, и при is_colored = True делается черно-белым\n","\n","        if isinstance(image, np.ndarray):\n","            if make_copy:\n","                self.img = image.copy()\n","            else:\n","                self.img = image\n","            if is_colored:\n","                self.img = self.make_black_and_white(self.img)\n","\n","        elif isinstance(image, type('')):\n","            t = cv2.imread(image)\n","\n","            if isinstance(t, type(None)):\n","                # изображение не удалось загрузить\n","                raise Exception(\"Некорректный путь к изображению\")\n","\n","            if is_colored:\n","                t = self.make_black_and_white(t)\n","\n","            self.img = t\n","\n","    def get_image(self) -> np.ndarray:\n","        return self.img\n","\n","    @staticmethod\n","    def make_black_and_white(img: np.ndarray) -> np.ndarray:\n","        # возвращает черно-белое изображение, соответствующее цветному img\n","        return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","\n","    def apply_filter(self, filter_type: FilterType, filter_size: int = 9) -> None:\n","        # модифицирует self.img, применяя к нему фильтр, соответствующий значению filter_type\n","\n","        def apply_gaussian(img, figure_size=9):\n","            return cv2.GaussianBlur(img, (figure_size, figure_size), 0)\n","\n","        def apply_median(img, figure_size=9):\n","            return cv2.medianBlur(img, figure_size)\n","\n","        if filter_type == FilterType.GAUSSIAN_FILTER:\n","            self.img = apply_gaussian(self.img, filter_size)\n","        elif filter_type == FilterType.MEDIAN_FILTER:\n","            self.img = apply_median(self.img, filter_size)\n","\n","    def apply_global_bin_filter(self, thresh: int = 220) -> None:\n","        # во все пикселы, значения которых больше порога thresh, устанавливаются значения 255\n","        # во все остальные -- 0\n","        mask = self.img >= thresh\n","        self.img[mask] = 255\n","        self.img[~mask] = 0\n","\n","    def apply_adaptive_bin_filter(self, mode: int = 0, **params):\n","        if mode == 0:\n","            self.img = apply_threshold(self.img, bradley_roth_threshold(self.img, **params))\n","        else:\n","            self.img = apply_threshold(self.img, singh_threshold(self.img))\n","\n","    def get_copy(self) -> np.ndarray:\n","        return self.img.copy()\n","\n","    def show(self):\n","        # выводит на экран изображение\n","        view_image(self.img, 'pic_handler_image')\n","\n","    @staticmethod\n","    def crop(img: np.ndarray, rect: Rect, make_copy: bool = False) -> np.ndarray:\n","        min_x, max_x = rect.left(), rect.right()\n","        min_y, max_y = rect.top(), rect.bottom()\n","\n","        res = img[min_y: max_y + 1, min_x: max_x + 1]\n","        return res.copy() if make_copy else res\n","\n","    @staticmethod\n","    def draw_pixels(rect: Rect, pixels: Set[Tuple[int, int]]) -> np.ndarray:\n","        # имеем pixels -- координаты закрашенных \"1\" точек на исходном изображении.\n","        # Метод рисует то, что должно быть в rect\n","        sh = rect.shape()\n","        dy, dx = -rect.top(), -rect.left()\n","        res = np.zeros((sh[1] + 1, sh[0] + 1), dtype=np.uint8)\n","        for x, y in pixels:\n","            res[y + dy, x + dx] = 1\n","\n","        return res\n","\n","    def make_zero_one(self) -> np.ndarray:\n","        # возвращает матрицу для бинарного изображения: 1, если пиксел не закрашен, иначе 0\n","        # Не вызывайте этот метод, если изображение не бинаризовано\n","        return (self.img == 0).astype(np.uint8)\n","\n","    @staticmethod\n","    def from_zero_one(mat: np.ndarray) -> np.ndarray:\n","        # mat: 1, если пиксел не закрашен, иначе 0\n","        return mat * 255\n","\n","    def draw_rect(self, rect: Rect, color: int = 0) -> None:\n","        left, right, top, bottom = rect.left(), rect.right(), rect.top(), rect.bottom()\n","        for x_static in (left, right):\n","            for y_dyn in range(top, bottom + 1):\n","                self.img[y_dyn, x_static] = color\n","\n","        for y_static in (top, bottom):\n","            for x_dyn in range(left, right + 1):\n","                self.img[y_static, x_dyn] = color\n","\n","    def __rebin(self) -> None:\n","        self.apply_global_bin_filter()\n","\n","    def resize(self, shape: Tuple[int, int]) -> None:\n","        self.img = resize(self.img, shape, preserve_range=True)\n","        self.__rebin()\n","\n","    def rescale(self, scale: float) -> None:\n","        self.img = rescale(self.img, scale)\n","        self.__rebin()\n","\n","    def exec_pipeline(self, pipeline: Callable, make_copy: bool=False, **params) -> PicHandler:\n","        if make_copy:\n","            ph = PicHandler(self.img)\n","        else:\n","            ph = self\n","        pipeline(ph, **params)\n","        return self\n","\n","    @staticmethod\n","    def crop_by_blank(img, side: Side | list[Side] = Side.all, blank_line: int=255, blank_delta=5) -> np.ndarray:\n","        x_min, y_min, y_max, x_max = 0, 0, *img.shape\n","\n","        if type(side) == list:\n","            if len(side) == 1:\n","                side = side[0]\n","\n","        if type(side) != list:\n","            if side == Side.all:\n","                side = [s for s in Side if s != Side.all]\n","            else:\n","                side = [side]\n","\n","        not_blank = np.abs(img - blank_line) >= blank_delta  # area where img pixels are not blank\n","        for s in side:\n","            \"\"\"\n","            match s:\n","                case Side.left: x_min = np.where(np.all(img != blank_line, axis=0))[0]\n","                case Side.right: x_max = np.where(np.all(img != blank_line, axis=0))[-1]\n","                case Side.left: y_min = np.where(np.all(img != blank_line, axis=1))[0]\n","                case Side.left: x_max = np.where(np.all(img != blank_line, axis=1))[-1]\"\"\"\n","            \n","            \n","            if s == Side.left: x_min = np.where(np.any(not_blank, axis=0))[0][0]\n","            elif s == Side.right: x_max = np.where(np.any(not_blank, axis=0))[-1][-1] + 1\n","            elif s == Side.top: y_min = np.where(np.any(not_blank, axis=1))[0][0]\n","            elif s == Side.bottom: y_max = np.where(np.any(not_blank, axis=1))[-1][-1] + 1\n","\n","        return img[y_min: y_max, x_min: x_max]\n"],"metadata":{"id":"vYxHR8J5Q14V","executionInfo":{"status":"ok","timestamp":1679326441171,"user_tz":-180,"elapsed":844,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["path = 'drive/MyDrive/Colab Notebooks/CTR advanced/align_model/'\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","class WriteHelper:\n","    SUPERSCRIPT: Set[str] = {s for s in 'бвёйАБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'}\n","    SUBSCRIPT: Set[str] = {s for s in 'дзруф'}\n","    alphabet: List[str] = [char for char in \"\"\"АБВГДЕЁЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯабвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\"\"]\n","    trivial_alphabet: List[str] = [char for char in \"\"\"абвгдеёжзийклмнопрстуфхцчшщъыьэюя\"\"\"]\n","    punctuation: List[str] = [char for char in \",.;!?\"]\n","    \n","    OUT_OF_LINE_HEIGHT: float = 0.9\n","\n","    with open(path + 'mean_symb_width', 'rb') as f:\n","        widths = pickle.load(f)\n","\n","    @staticmethod\n","    def get_symb_widths(symbols: List[str], word_width: int) -> List[int]:\n","        symb_w_relative = [WriteHelper.__get_symb_w(s) * WriteHelper.__get_coef(s) for s in symbols]\n","        word_w_relative = sum(symb_w_relative)\n","        scale = word_width / word_w_relative\n","        symb_w = [int(sw * scale) for sw in symb_w_relative]\n","        return symb_w\n","\n","\n","    @staticmethod\n","    def __get_coef(s: str) -> float:\n","        if WriteHelper.have_script(s, True) or WriteHelper.have_script(s, False):\n","            return 1.55  # TODO : remove magic numbers\n","        else:\n","            return 1\n","\n","    @staticmethod\n","    def __get_symb_w(symb: str) -> float:\n","        if symb in WriteHelper.widths.keys():\n","            return WriteHelper.widths[symb]\n","        else:\n","            return 0.05  # пунктуация; TODO : remove magic numbers\n","\n","    @staticmethod\n","    def __have_script(s: str, charset: Set[str]) -> bool:\n","        for char in s:\n","            if char in charset:\n","                return True\n","        return False\n","\n","    @staticmethod\n","    def have_script(s: str, superscript: bool) -> bool:\n","        if superscript:\n","            return WriteHelper.__have_script(s, WriteHelper.SUPERSCRIPT)\n","        else:\n","            return WriteHelper.__have_script(s, WriteHelper.SUBSCRIPT)\n","\n","    @staticmethod\n","    def is_space(symb: str) -> bool:\n","        return symb in (' ', '\\t', '\\n')\n","\n","    @staticmethod\n","    def to_hdf5_key(symb) -> str:\n","        if symb in WriteHelper.punctuation:\n","            return '<%s>' % symb\n","        else:\n","            return symb\n","\n","\n","alphabet = [' '] + WriteHelper.trivial_alphabet + list(\"\"\",.:!?-—;()\"\"\")#alphabet\n","NULL_SYMB = '^'\n","advanced_alf = [s.upper() for s in WriteHelper.trivial_alphabet]\n","\n","\n","def char_to_num(char: str) -> str:\n","    if char in alphabet:\n","        return alphabet.index(char)\n","    elif char == NULL_SYMB:\n","        return len(alphabet)\n","    else:\n","        return len(alphabet) + 1 + advanced_alf.index(char)\n","\n","\n","def num_to_char(num: Union[str, int]) -> str:\n","    if int(num) < len(alphabet):\n","        return alphabet[int(num)]\n","    elif int(num) == len(alphabet):\n","        return NULL_SYMB\n","    else:\n","        return advanced_alf[int(num) - len(alphabet) - 1]\n","\n","\n","def str_to_array(string: str, lowercase: bool = True) -> np.ndarray:\n","    # в датасете есть интересные символы...\n","    string = string.replace('…', '...').replace('Қ', 'К').replace('o', 'о')\\\n","        .replace('H', 'Н').replace('Ө', 'О').replace('қ', 'к').replace('–', '—').replace('ө', 'о')#.lower()\n","    if lowercase:\n","        string = string.lower()\n","    return np.array([char_to_num(ch) for ch in string])\n","\n","\n","def get_data(data_path):\n","    f = h5py.File(data_path, 'r')\n","    images, labels = f['images'][:], f['labels'][:]\n","    f.close()\n","    return images, labels\n","\n","\n","def collate_fn(batch):\n","    max_len = len(max(batch, key=lambda e: len(e[1]))[1])\n","    labels = torch.zeros((len(batch), max_len), dtype=torch.float16)\n","\n","    lens = np.array([len(e[1]) for e in batch])\n","    to_pad = max_len - lens\n","\n","    for i in range(len(batch)):\n","        target = batch[i][1]\n","        labels[i] = torch.cat((tensor(target), torch.zeros(to_pad[i])))\n","\n","    X = torch.as_tensor(np.array([e[0] for e in batch]), dtype=torch.float32)\n","    return X.view(-1, 1, X.shape[1], X.shape[2]), \\\n","        (labels, torch.as_tensor(lens, dtype=torch.int32))\n","\n","\n","def get_loaders(batch_size, data_path, num_workers, lowercase_labels: bool = True):\n","    num_workers = int(num_workers)  # часто получаем строку\n","    images, labels = get_data(data_path)\n","    targets = [str_to_array(str(string, 'utf8'), lowercase_labels) for string in labels]\n","\n","    data = list(zip(images, targets))  # сортировать список по возрастанию длин слов не нужно, это может\n","                                       # ухудшить результаты\n","    train_data, test_data = train_test_split(data, train_size=0.9, shuffle=True, random_state=42)\n","\n","    train_loader = DataLoader(\n","        train_data, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers)\n","\n","    test_loader = DataLoader(\n","        test_data, batch_size=batch_size, collate_fn=collate_fn, num_workers=num_workers)\n","\n","    return {\n","        'train': train_loader,\n","        'val': test_loader\n","    }\n","\n","\n","\n","HIDDEN_SIZE = 256\n","num_classes = OUTPUT_DIM = len(alphabet) + 1\n","BIDIRECTIONAL = True\n","BLANK = num_classes - 1\n","\n","\n","class GatedConvolution(nn.Module):\n","    def __init__(self, img_shape, **conv_args):\n","        super(GatedConvolution, self).__init__()\n","        self.conv = nn.Conv2d(conv_args['out_channels'], conv_args['out_channels'], kernel_size=conv_args['kernel_size'], padding=conv_args['padding'])\n","\n","    def forward(self, x):\n","        return torch.tanh(self.conv(x)) * x\n","\n","\n","def extended_conv_layer(img_shape, **conv_params):\n","    return nn.Sequential(\n","        *[\n","           nn.Conv2d(**conv_params, ),\n","           nn.PReLU(),\n","           nn.BatchNorm2d(conv_params['out_channels']),\n","           GatedConvolution(img_shape, **conv_params)\n","        ]\n","    )\n","\n","\n","class RecogModel3(nn.Module):\n","    def __init__(self):\n","        super(RecogModel3, self).__init__()\n","\n","        self.encoder = nn.Sequential(\n","            *[\n","                nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3, padding=1),\n","                # 16x128x1024\n","                extended_conv_layer(img_shape=(128, 1024), in_channels=8, out_channels=16, kernel_size=3, padding=1),\n","                nn.Dropout2d(p=0.2),\n","                # extended_conv_layer(img_shape=(128, 1024), in_channels=16, out_channels=16, kernel_size=3, padding=1),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","                # 32x64x512\n","                # nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n","                extended_conv_layer(img_shape=(64, 512), in_channels=16, out_channels=32, kernel_size=3, padding=1),\n","                # extended_conv_layer(img_shape=(64, 512), in_channels=16, out_channels=32, kernel_size=3, padding=1),\n","                # nn.Dropout2d(p=0.2),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","                # 64x32x256\n","                # nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n","                extended_conv_layer(img_shape=(32, 256), in_channels=32, out_channels=32, kernel_size=3, padding=1),\n","                # extended_conv_layer(img_shape=(25, 90), in_channels=BB_OUTPUT, out_channels=BB_OUTPUT, kernel_size=3, padding=1),\n","                nn.Dropout2d(p=0.2),\n","                nn.MaxPool2d(kernel_size=2, stride=2),\n","\n","                # -> 32x16x128\n","\n","                # 36x45x10\n","                # extended_conv_layer(img_shape=(10, 45), in_channels=BB_OUTPUT, out_channels=BB_OUTPUT, kernel_size=3, padding=1),\n","                # nn.Conv2d(in_channels=BB_OUTPUT, out_channels=BB_OUTPUT, kernel_size=(12, 1)),\n","                # nn.Conv2d(in_channels=BB_OUTPUT, out_channels=BB_OUTPUT, kernel_size=(1, 1)),\n","                # nn.Dropout2d(p=0.2),\n","                # 36x45x1\n","                # nn.Flatten(),\n","                # nn.LazyLinear(out_features=HIDDEN_STATE_DIM),\n","                # nn.LazyLinear(out_features=NUM_OF_BB*BB_OUTPUT),\n","            ]\n","        )\n","\n","        self.rnn = nn.Sequential(\n","            *[\n","                nn.LSTM(input_size=32 * 16, hidden_size=HIDDEN_SIZE, num_layers=1,\n","                       bidirectional=BIDIRECTIONAL, batch_first=True, )  # dropout=0.2)\n","            ]\n","        )\n","\n","        self.output_decoder = nn.Sequential(\n","            *[\n","                nn.Linear(in_features=HIDDEN_SIZE * (1 + int(BIDIRECTIONAL)),\n","                          out_features=OUTPUT_DIM),\n","                # nn.Dropout(p=0.1),\n","                nn.LogSoftmax(dim=-1)\n","            ]\n","        )\n","\n","        self.init_conv()\n","\n","    def forward(self, x: Tensor):\n","        enc = self.encoder(x)  # BATCH_SIZE x 32 x 16 x 128\n","        enc = enc.permute(0, 3, 1, 2).view(-1, 128, 16 * 32)\n","\n","        response, (hn, cn) = self.rnn(enc)\n","        return self.output_decoder(response)\n","\n","    def init_conv(self):\n","        \"\"\"\n","        Initialize convolution parameters.\n","        \"\"\"\n","        for c in self.children():\n","            if isinstance(c, nn.Conv2d) or isinstance(c, nn.Conv3d):\n","                nn.init.xavier_uniform_(c.weight)\n","                nn.init.constant_(c.bias, 0.)\n","\n","\n","class LModel(pl.LightningModule):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.model = model\n","        self.loss_obj = nn.CTCLoss(blank=BLANK, zero_infinity=True, reduction='mean')\n","\n","    def loss_fn(self, outputs, targets):\n","        target_labels, target_lens = targets\n","        input_lens = torch.full((len(target_lens),), 128, dtype=torch.int32) \n","        outputs = outputs.permute(1, 0, 2)\n","        return self.loss_obj(outputs, target_labels, input_lens, target_lens)\n","    \n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def training_step(self, batch, batch_idx):\n","        X, y = batch \n","        pred = self.model(X)\n","        loss = self.loss_fn(pred, y)\n","        self.log('train_loss', loss.item(), reduce_fx='mean')\n","        #tb_logs = {'val_loss': loss}\n","        return {\n","            'loss': loss,\n","        }\n","\n","    \"\"\"def test_step(self, batch, batch_idx):\n","        x, y = batch\n","        pred = self.model(x)\n","        loss = self.loss_func(pred, y)\n","        self.log('test_loss', loss.item(), prog_bar=True, reduce_fx=\"mean\")\n","        output = dict({\n","            'test_loss': loss,\n","        })\n","        return output\"\"\"\n","    \n","    def validation_step(self, batch: Tuple[Tensor, Tensor], _batch_index: int) -> None:\n","        inputs_batch, labels_batch = batch\n","\n","        outputs_batch = self(inputs_batch)\n","        loss = self.loss_fn(outputs_batch, labels_batch)\n","\n","        self.log('val_loss', loss.item(), reduce_fx=\"mean\")\n","        return {\n","            'val_loss': loss,\n","        }\n","    \n","    def validation_epoch_end(self, outputs):\n","        avg_loss = torch.tensor([x[\"val_loss\"] for x in outputs]).mean()  # на самом деле уже не нужно усреднять\n","        tb_logs = {'val_loss': avg_loss}\n","        return {'val_loss': avg_loss, 'log': tb_logs}\n","    \n","    def configure_optimizers(self):\n","        optimizer = torch.optim.RMSprop(self.parameters(), lr=1e-3)\n","        sched = ReduceLROnPlateau(optimizer, verbose=True, patience=3)\n","        return {\n","            \"optimizer\": optimizer, \n","            \"lr_scheduler\": {\n","                \"scheduler\": sched,\n","                \"monitor\": \"val_loss\"}\n","        }\n"],"metadata":{"id":"hrJxNi5w6krW","executionInfo":{"status":"ok","timestamp":1679325880859,"user_tz":-180,"elapsed":1213,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 1\n","dataloader = get_loaders(BATCH_SIZE, path + 'data3000.hdf5', 1, False)['train']"],"metadata":{"id":"FyfdNVms7d8T","executionInfo":{"status":"ok","timestamp":1679325892254,"user_tz":-180,"elapsed":11400,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from skimage.transform import rescale\n","\n","\n","default_shape = (128, 1024)\n","BATCH_SIZE = 32\n","DEFAULT_SAMPLES = 1000\n","DSHAPE = 250\n","IMG_DOWNSCALE = 2\n","big_img_h = default_shape[0]\n","new_h = big_img_h / IMG_DOWNSCALE\n","SHAPE_FOR_SYMBOL = (new_h, new_h * 2) #(128, 256)\n","\n","downcale_k = 1 / IMG_DOWNSCALE\n","\n","\n","def decode(y_pred: str) -> str:\n","    actual_symb = res = y_pred[0]\n","\n","    for si in range(len(y_pred)):\n","        s = y_pred[si]\n","        if s == actual_symb:\n","            continue\n","        elif s == NULL_SYMB:\n","            actual_symb = ''\n","            continue\n","\n","        if s != actual_symb:\n","            res += s\n","            actual_symb = s\n","    \n","    return res.replace(NULL_SYMB, '')\n","\n","\n","def append_to_dict(db_dict, k, v, sizes):\n","    if k not in sizes.keys():\n","        sizes[k] = 0\n","        db_dict.create_dataset(k, (DEFAULT_SAMPLES, *SHAPE_FOR_SYMBOL), compression=\"gzip\", \n","                               compression_opts=4, maxshape=(None, *default_shape))\n","\n","    if sizes[k] == len(db_dict[k]):\n","        # нужно увеличить\n","       db_dict[k].resize((len(db_dict[k]) + DSHAPE, *default_shape))\n","\n","    v = rescale(v, downcale_k)\n","\n","    db_dict[k][sizes[k], : v.shape[0], : v.shape[1]] = v\n","    sizes[k] += 1\n","\n","\n","def collate_dict(db_dict, sizes):\n","    for k in sizes.keys():\n","        real_size = sizes[k]\n","        db_dict[k].resize((real_size, *default_shape))"],"metadata":{"id":"bukLjinFBAeB","executionInfo":{"status":"ok","timestamp":1679325892256,"user_tz":-180,"elapsed":59,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def improve_symb_borders(symbol_widths: list[tuple[str, int]], word_width: int, \n","                         max_increase=10, max_decrease=5, begin_w=1, end_w=2) \\\n","                            -> list[tuple[int, int]]:\n","\n","    # возвращает список границ для каждого символа\n","    # мы считаем, что сеть +- правильно нашла центры областей символов, но мб\n","    # не очень точно определила границы каждого символа; уточним их на основе \n","    # априорной информации о ширине символов\n","    bw, ew = begin_w / (begin_w + end_w), end_w / (begin_w + end_w)\n","    res = []\n","\n","    aprior_precision = WriteHelper.get_symb_widths(\n","        [s for s, w in symbol_widths], word_width\n","    )\n","    \n","    tstart = 0\n","    for (symb, width), aprior_width in zip(symbol_widths, aprior_precision):\n","        end = tstart + width\n","        delta = aprior_width - width\n","        modify = 0\n","        if delta > 0:\n","            modify = min(delta, max_increase)\n","        elif delta < 0:\n","            modify = max(delta, -max_decrease) \n","        \n","        res.append(\n","            (\n","                #int(tstart),\n","                int(max(tstart - modify * bw, 0)), \n","                int(min(end + modify * ew, word_width - 1))\n","            )\n","        )\n","        tstart = end\n","\n","    return res"],"metadata":{"id":"lcdglHnYel4a","executionInfo":{"status":"ok","timestamp":1679325892258,"user_tz":-180,"elapsed":57,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["model = LModel.load_from_checkpoint(path + 'model3000.ckpt', map_location=device, model=RecogModel3())\n","model.eval()\n","sizes = dict()\n","db_path_name = path + 'symb_db3000.hdf5'\n","\n","empty_space_threshold = 0.07\n","\n","with h5py.File(db_path_name, 'w') as f:\n","\n","    for X, y in tqdm(dataloader):\n","        y_pred = model(X.to(device))\n","\n","        for i in range(len(X)):\n","            # рассмотрим одно изображение в батче\n","            w = X[i].shape[-1]\n","            scale = w / y_pred[i].shape[0]\n","            \n","            y_pred_str = ''.join([num_to_char(n) for n in y_pred[i].argmax(dim=1)])\n","            y_true_str = ''.join([num_to_char(n) for n in y[0][i][: y[1][i]]])\n","\n","            if decode(y_pred_str) != y_true_str.lower():\n","                continue\n","\n","            start_symb = 0\n","            end_symb = 0\n","            tstart = y_true_str[0].lower()\n","            cnt_symb = 0\n","            \n","            tmp_img = X[i, 0].cpu().numpy().copy()\n","            symbol_widths = []\n","\n","            img_hor_cropped = PicHandler.crop_by_blank(\n","                X[i, 0].cpu().numpy(), blank_line=0, blank_delta=empty_space_threshold\n","            )\n","\n","            sum_w = 0\n","            while end_symb < len(y_pred_str):\n","                if tstart != y_pred_str[end_symb] and (y_pred_str[end_symb] != NULL_SYMB or end_symb == len(y_pred_str) - 1):\n","                    # мы нашли границу \n","                    next_width = end_symb - start_symb\n","                    real_width = min(scale * next_width, \n","                                     img_hor_cropped.shape[-1] - sum_w)\n","                    symbol_widths.append((tstart, real_width))\n","                    sum_w += real_width\n","\n","                    start_symb = end_symb\n","                    tstart = y_pred_str[start_symb]  # точно != NULL_SYMB          \n","                    cnt_symb += 1\n","\n","                end_symb += 1\n","            \n","            # имеем массив symbol_widths -- предположительная ширина каждого \n","            # символа в слове\n","            borders = improve_symb_borders(\n","                symbol_widths, img_hor_cropped.shape[1],\n","                max_increase = 10, max_decrease = 12\n","            )\n","            for symb, (l, r) in zip(y_true_str, borders):\n","                if WriteHelper.is_space(symb):\n","                    continue\n","                img = X[i, 0, :, l: r].cpu().numpy()\n","                try:\n","                    crop_res = PicHandler.crop_by_blank(img, [Side.top, Side.bottom], blank_line=0, blank_delta=empty_space_threshold)\n","                except:\n","                    # в окно по какой-то причине попало пустое пространство\n","                    continue\n","                img = (1 - crop_res) * 255\n","                if min(crop_res.shape) <= 4: \n","                    continue\n","                append_to_dict(f, WriteHelper.to_hdf5_key(symb), img, sizes)\n","                '''tmp_img [:, l] = 0.7\n","                tmp_img [:, r] = 0.8\n","                plt.text(l, -10, 'b')\n","                plt.text(r, -2, 'e')'''\n","\n","            #plt.imshow(tmp_img)\n","\n","    collate_dict(f, sizes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["c4cc201b5e874f50b4d465b466602a49","e0255e6687ad47b8b9fb70a04b960913","4bbae8f4ccfd4555b7f94cc5c7fe09ee","00484eaea7c146a2b4927d3f88c7455b","2330944dcd7141b8a7bd93b18e7b9322","7c5c3007b45f4bb6861271d7d4a4bdac","d8787969600a4c648a0409fe47150ea4","7e1448eeda484a51990ccd2aec3d3675","141dee8fd8d4429c8a30449731528bc7","854ca7cc836a4cd4a77754c009c6dd0d","2889617388634a38bb72d5d54df24c81"]},"id":"FNWbhlvZ2ZSz","executionInfo":{"status":"ok","timestamp":1679327209995,"user_tz":-180,"elapsed":714754,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"9d59aa8b-0cb0-4944-fef6-a760a078bbb0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.9.3 to v2.0.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file drive/MyDrive/Colab Notebooks/CTR advanced/align_model/model3000.ckpt`\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2700 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4cc201b5e874f50b4d465b466602a49"}},"metadata":{}}]},{"cell_type":"code","source":["plt.imshow(img)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"id":"hqKFQ6qhDBo9","executionInfo":{"status":"ok","timestamp":1679326202127,"user_tz":-180,"elapsed":602,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"c75c8f1e-f92e-4f11-9849-2a23d0bae487"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f4c2c5a9820>"]},"metadata":{},"execution_count":11},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["borders"],"metadata":{"id":"82BDDuAw47WB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679326314396,"user_tz":-180,"elapsed":16,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"43ed83f3-cb8e-4043-d64f-f5c7d56cc102"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 86), (76, 110), (100, 158), (148, 190), (188, 730)]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["y_true_str"],"metadata":{"id":"Hc0B16rSCy_A","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1679326241264,"user_tz":-180,"elapsed":282,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"0c544f11-bb4b-4fd5-f20b-7ac8e7cd209e"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'меня.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["y_pred_str"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"phFRQpMUpN6G","executionInfo":{"status":"ok","timestamp":1679326250973,"user_tz":-180,"elapsed":402,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}},"outputId":"20145722-991e-4c71-f1e5-9537acdce646"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'м^^^^^^^^^е^^нн^^^^я^^^.^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["!cp symb_db3000.hdf5 drive/MyDrive/'Colab Notebooks'/'CTR advanced'/align_model/symb_db3000.hdf5"],"metadata":{"id":"UWUFybl6sE9x","executionInfo":{"status":"ok","timestamp":1679327222075,"user_tz":-180,"elapsed":909,"user":{"displayName":"Stepan Chernikov","userId":"11416067074158210875"}}},"execution_count":19,"outputs":[]}]}